# Read messages from message queue
input {
    kafka { 
        bootstrap_servers => "kafka:9092"
        topics =>  [ "datapipeline-dataset" ]
        group_id => "logstash-consumer"
        client_id => "logstash-consumer"
        auto_offset_reset => "earliest"
        codec => "json"
    }
} # end of input block

# Transform the dataset per requirements
filter {
    # Parse message from Kafka and save to event field
    json {
        source => "message"
        target => "event"
    }

    # Add, remove, and copy fields per requirement
    mutate {  
        add_field => { "sourcetype" => "nginx" }
        add_field => { "index" => "nginx" }
        add_field => { "[fields][region]" => "us-west-1" }
        add_field => { "[fields][assetid]" => "8972349837489237" }        
        remove_field => [ "[event][original]", "[event][time]", "log", "@version", "message", "host" ]   
        copy => { "[event][time]" => "time" }
    }

    # Convert time field to date field type
    date {
        match => [ "time", "dd/MMM/yyyy:HH:mm:ss Z" ]
        target => "time"
    }

    # Change format of time field to epoch
    ruby { 
        code => "event.set('time', event.get('time').to_i)" 
    }

    # Change time field to date field type to enable use as @timefield in OpenSearch Dashboards
    date {
        match => [ "time", "UNIX" ]
        target => "@timestamp"
    }
} # end of filter block

# Produce the dataset to message queue
output {
    opensearch {
        hosts => ["https://opensearch-node1:9200","https://opensearch-node2:9200"]
        index => "nginx"
        user => "${opensearch_user}"
        password => "${opensearch_pwd}"
        ssl => true
        ssl_certificate_verification => false
    }      
} # end of output block
